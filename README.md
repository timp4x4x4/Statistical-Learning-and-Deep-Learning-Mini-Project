# Statistical-Learning-and-Deep-Learning-Mini-Project

# 房價預測專案

**專案名稱**: 統計學習與深度學習房價預測專案  
**學號**: B11705051  
**學系**: 資管三  
**姓名**: 陳奕廷

## 專案背景

本專案基於 Kaggle 提供的房價預測資料集，利用機器學習的方法進行房價預測，並採用 RMSE（Root Mean Squared Error）作為評估指標。選擇 XGBoost 作為主要模型，是因為其在大規模資料處理時具有高效性和優越性能，相較於 Random Forest，XGBoost 的訓練速度更快。本專案主要聚焦於資料探索、特徵工程、模型訓練與優化等流程，並通過多次迭代提升模型的準確度。

## 機器學習流程

### 1. 資料探索與分析
- 讀取 Kaggle 提供的資料集，並對資料進行基本清理和初步觀察。
- 分析各特徵的分佈與關聯性，探索影響房價的可能因子。

### 2. 初步模型測試
- 使用簡單的線性回歸模型進行初步預測，了解資料的複雜度與特徵表現。
- 採用基礎的 XGBoost 模型進行初步訓練，並評估模型預測結果。

### 3. 新增特徵工程
- 根據對資料的理解，新增一些重要的特徵，如屋齡、One-Hot Encoding 等。
- 對特徵進行轉換與交互（例如：平方或特徵組合），以提高模型表現。

### 4. 模型訓練與初步結果
- 訓練 XGBoost 模型，初步預測結果 RMSE 約為 40,000。

## 後續優化與迭代流程

### 1. 特徵重要性分析
- 通過觀察模型的特徵重要性排序，找出對預測結果影響最大的特徵。

### 2. 問題發現與特徵創造
- 根據特徵表現，設計新的特徵。例如：
  - 高重要性特徵之間的交互特徵。
  - 特徵的平方或對數轉換。
  - 提升模型對資料的擬合能力。

### 3. 超參數調整
- 使用 RandomizedSearchCV 進行超參數調整，優化模型性能，調整的超參數包括：
  - 樹的深度（`max_depth`）。
  - 學習率（`learning_rate`）。
  - 子樣本比例（`subsample`）。
  - 正則化參數等。

### 4. 模型評估與提交
- 訓練優化後的 XGBoost 模型，進行最終預測，並將結果上傳至 Kaggle 平台。

## 資料分析

在資料分析部分，主要進行了以下步驟：

- **單價元平方公尺的分佈**: 資料顯示，台北市的房價呈現正常分佈，並無明顯的偏移問題。
  
- **區域差異分析**: 可以看到，台北市的房價在不同區域之間存在顯著差異，例如大安區的房價較萬華區高。這些差異反映了現實中的市場價格。

- **座標與顏色分析**: 透過地圖視覺化，發現相鄰房價的價格較為相似，這顯示出地理位置對房價的影響。

- **Anchor 特徵工程**: 使用 Anchor 的方式對區域內所有資料點進行平均，作為錨點，計算與各區域的距離，這有助於提升預測準確度。

- **其他影響特徵**: 包括屋齡、區域環境評估（如附近的機能設施和超商等）、鄰里便捷性指數（如地鐵站等），這些特徵對預測有顯著影響。

### 特徵之間的關聯性
- 透過資料視覺化，我發現屋齡與房價之間存在反向關係，屋齡越高，價格越低。

## 處理 One-Hot 編碼問題

由於資料中路名種類繁多，進行 One-Hot Encoding 時會產生過多的特徵，這會導致模型複雜度過高，並影響模型效能。為了解決這一問題，我採用了 Target Encoding。

### Target Encoding 方法：
- 針對每條路名，計算其在訓練資料中的平均房價（`y_train`），並將此平均價格賦予相應資料點。例如，若某資料點的路名是“內湖路一段”，且其平均價格為 `a`，則將 `a` 賦給該資料點的“路名_平均價格”特徵。
- 在測試資料上，也同樣使用預處理的字典來分配相應的價格值。若測試資料中出現未見過的路名，則可以給予該資料點總平均價格。

### 在其他特徵上的應用：
- 除了路名，我也在其他特徵上使用了 Target Encoding，避免 One-Hot 編碼帶來的過多特徵問題。

## 優化與迭代

在模型訓練後，我會生成特徵重要性圖，並針對關聯性較強的特徵進行交互，創造新特徵，並反覆優化模型，從而提升預測效果，避免過度擬合（Overfitting）。

---
